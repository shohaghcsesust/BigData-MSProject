{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fe0607",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-22T18:25:21.579625Z",
     "iopub.status.busy": "2025-10-22T18:25:21.579253Z",
     "iopub.status.idle": "2025-10-22T18:25:35.311604Z",
     "shell.execute_reply": "2025-10-22T18:25:35.310355Z"
    },
    "papermill": {
     "duration": 13.740181,
     "end_time": "2025-10-22T18:25:35.313370",
     "exception": false,
     "start_time": "2025-10-22T18:25:21.573189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded workbook: /kaggle/input/proserv-360-uiu-conbined-dataset-kader-and-kabir/Prediction Data.xlsx\n",
      "Using sheet: Sheet1\n",
      "\n",
      "=== Step 2: Select features and targets ===\n",
      "Feature columns: ['Modality', 'Problem Category', 'Product Desc', 'LCT', 'Parts Y/N']\n",
      "Target columns: ['Time to Return', 'Span']\n",
      "Dropped rows with missing targets: 0\n",
      "\n",
      "Pipeline created successfully!\n",
      "Train size: 2794 rows, Test size: 699 rows\n",
      "\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "=== Test Performance ===\n",
      "Time to Return: MAE=1.0243, RMSE=2.3395, R²=-0.0485\n",
      "Span: MAE=4.8330, RMSE=7.8640, R²=-0.0679\n",
      "\n",
      "Sample predictions (first 10 rows):\n",
      "     Modality  Problem Category                   Product Desc         LCT  \\\n",
      "1945       CT             Other     REVOLUTION ACTS SVCT INDIA     Chennai   \n",
      "3362       CT     Imaging Issue             REVOLUTION HD 2000      Cochin   \n",
      "2649       CT     Imaging Issue   REVOLUTION ACTS SVCT Z4G4 IN        Pune   \n",
      "1880       CT             Other     REVOLUTION ACTS SVCT INDIA   Bangalore   \n",
      "2768       CT             Other   REVOLUTION ACTS SVCT Z4G4 IN     Lucknow   \n",
      "785        CT             Other                   OPTIMA CT660   Bangalore   \n",
      "3306       CT     Imaging Issue    REVOLUTION EVO 3.7 MID HINO  Chandigarh   \n",
      "1283       CT     Imaging Issue          REVO MAXIMA MID INDIA      Cochin   \n",
      "678        CT     Imaging Issue  OPTIMA 660 M40 GT1700 BEIJING   Ahmedabad   \n",
      "3072       CT  Mechanical Issue    REVOLUTION EVO 3.6B MID BJG    Delhi DI   \n",
      "\n",
      "          Parts Y/N  Actual Time to Return  Pred Time to Return  Actual Span  \\\n",
      "1945     With Parts               0.138681             0.243601     7.165301   \n",
      "3362  Without Parts               0.000000             0.012398    11.919676   \n",
      "2649  Without Parts               0.229502             0.219188     1.012535   \n",
      "1880  Without Parts               0.039664             2.215506     1.202269   \n",
      "2768  Without Parts               0.229236             0.815030     1.099965   \n",
      "785   Without Parts               0.009711             0.081482     0.453241   \n",
      "3306  Without Parts               0.015058             0.254915     4.452488   \n",
      "1283  Without Parts               0.044722             0.010988     5.193993   \n",
      "678      With Parts               0.294653             0.536399     8.949815   \n",
      "3072     With Parts               0.970787             0.443076     5.953287   \n",
      "\n",
      "      Pred Span  \n",
      "1945   8.813212  \n",
      "3362   7.283609  \n",
      "2649   2.795234  \n",
      "1880   5.975072  \n",
      "2768   6.231627  \n",
      "785    5.207787  \n",
      "3306   5.789129  \n",
      "1283   2.335084  \n",
      "678    7.992754  \n",
      "3072   7.621132  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Machine Learning Training Script\n",
    "# Predicts \"Time to Return\" and \"Span\" using selected fields\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# === Step 1: Load data ===\n",
    "file_path = \"/kaggle/input/proserv-360-uiu-conbined-dataset-kader-and-kabir/Prediction Data.xlsx\"\n",
    "excel = pd.ExcelFile(file_path)\n",
    "sheet_name = excel.sheet_names[0]\n",
    "print(f\"Loaded workbook: {file_path}\")\n",
    "print(f\"Using sheet: {sheet_name}\")\n",
    "\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# Ordered fields (1-based index)\n",
    "ordered_fields = [\n",
    "    'Parts Y/N', 'Service Region', 'Opened', 'Initial Closed Date', 'Month',\n",
    "    'Span', 'Owner', 'Owner Name', 'Type', 'LCT', 'Zone', 'Entitlement',\n",
    "    'Asset Number', 'Equipment Return to Customer', 'Time to Return', 'SR Type',\n",
    "    'Status', 'Description', 'Problem Category', 'Modality', 'MOD',\n",
    "    'Customer Equipment Status', 'Product Desc', 'Customer Name', 'Customer #',\n",
    "    'System Id', 'Product', 'Model', 'Processed', 'Current Equipment Status',\n",
    "    'Ship To Country', 'SR Sub Type', 'Closed', 'Source Sub Type'\n",
    "]\n",
    "\n",
    "# Input fields (20, 19, 23, 10, 1)\n",
    "input_positions = [20, 19, 23, 10, 1]\n",
    "X_cols = [ordered_fields[i-1] for i in input_positions]\n",
    "\n",
    "# Target fields (15, 6)\n",
    "target_positions = [15, 6]\n",
    "y_cols = [ordered_fields[i-1] for i in target_positions]\n",
    "\n",
    "print(\"\\n=== Step 2: Select features and targets ===\")\n",
    "print(f\"Feature columns: {X_cols}\")\n",
    "print(f\"Target columns: {y_cols}\")\n",
    "\n",
    "# Keep only necessary columns\n",
    "df_small = df[X_cols + y_cols].copy()\n",
    "\n",
    "# === Step 3: Basic cleaning ===\n",
    "for c in y_cols:\n",
    "    df_small[c] = pd.to_numeric(df_small[c], errors='coerce')\n",
    "\n",
    "before_rows = len(df_small)\n",
    "df_small = df_small.dropna(subset=y_cols)\n",
    "after_rows = len(df_small)\n",
    "print(f\"Dropped rows with missing targets: {before_rows - after_rows}\")\n",
    "\n",
    "X = df_small[X_cols]\n",
    "y = df_small[y_cols]\n",
    "\n",
    "# === Step 4: Build preprocessing & model ===\n",
    "cat_features = list(range(len(X_cols)))\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[(\"cat\", OneHotEncoder(handle_unknown='ignore', sparse=False), cat_features)]\n",
    ")\n",
    "\n",
    "base_model = RandomForestRegressor(n_estimators=300, n_jobs=-1, random_state=42)\n",
    "model = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"regressor\", MultiOutputRegressor(base_model))\n",
    "])\n",
    "\n",
    "print(\"\\nPipeline created successfully!\")\n",
    "\n",
    "# === Step 5: Train/test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Train size: {X_train.shape[0]} rows, Test size: {X_test.shape[0]} rows\")\n",
    "\n",
    "# === Step 6: Train model ===\n",
    "print(\"\\nTraining model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# === Step 7: Evaluate performance ===\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Test Performance ===\")\n",
    "for j, target in enumerate(y_cols):\n",
    "    mae = mean_absolute_error(y_test.iloc[:, j], y_pred[:, j])\n",
    "    rmse = mean_squared_error(y_test.iloc[:, j], y_pred[:, j], squared=False)\n",
    "    r2 = r2_score(y_test.iloc[:, j], y_pred[:, j])\n",
    "    print(f\"{target}: MAE={mae:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
    "\n",
    "# === Step 8: Sample predictions ===\n",
    "print(\"\\nSample predictions (first 10 rows):\")\n",
    "sample = X_test.iloc[:10].copy()\n",
    "sample[f\"Actual {y_cols[0]}\"] = y_test.iloc[:10, 0].values\n",
    "sample[f\"Pred {y_cols[0]}\"] = y_pred[:10, 0]\n",
    "sample[f\"Actual {y_cols[1]}\"] = y_test.iloc[:10, 1].values\n",
    "sample[f\"Pred {y_cols[1]}\"] = y_pred[:10, 1]\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05832a90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T18:25:35.322809Z",
     "iopub.status.busy": "2025-10-22T18:25:35.322389Z",
     "iopub.status.idle": "2025-10-22T18:25:55.538821Z",
     "shell.execute_reply": "2025-10-22T18:25:55.537569Z"
    },
    "papermill": {
     "duration": 20.223049,
     "end_time": "2025-10-22T18:25:55.540518",
     "exception": false,
     "start_time": "2025-10-22T18:25:35.317469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded shape: (3119, 40)\n",
      "[INFO] Columns (first 25): ['City', 'OPH_Modality', 'OPH_Product_Line', 'OPH_Segment', 'OPH_Family_Name', 'OPH_Planning_Name', 'OPH_Product_Group', 'OPH_PSI_Description', 'product_identifier', 'OPH_Product_Set_Desc', 'site_name', 'ucm_customer_id', 'asset_installed_city', 'SR_No', 'SR_Type_STD', 'SR_Owner_SSO', 'SR_Problem_Description', 'fcr_status', 'SR_Open_Date', 'SR_Close_Date', 'SR_Total_Labor_Hours', 'SR_Total_Travel_Hours', 'SR_Total_Part_Quantity', 'Asset_System_ID', 'Asset_Serial_Number']\n",
      "[INFO] Using timestamp column: SR_Open_Date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/27277503.py:39: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_13/27277503.py:71: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  flag = flag | frame[\"SR_Type\"].astype(str).str.contains(r\"(corrective|breakdown)\", case=False, regex=True)\n",
      "/tmp/ipykernel_13/27277503.py:80: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  not_pm = ~text_join.str.contains(r\"(pm|preventive|installation|training|upgrade)\", case=False, regex=True)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n",
      "  return op(a, b)\n",
      "/tmp/ipykernel_13/27277503.py:107: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(ASSET_COL, group_keys=False).apply(synthesize_time_within_asset)\n",
      "/tmp/ipykernel_13/27277503.py:125: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  labeled = df.groupby(ASSET_COL, group_keys=False).apply(build_next_failure_labels)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n",
      "  return op(a, b)\n",
      "/tmp/ipykernel_13/27277503.py:145: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  f[\"time_since_last_failure\"] = f.groupby(ASSET_COL, group_keys=False).apply(time_since_last_failure)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Label stats (incl. censored):\n",
      "count    2833.000000\n",
      "mean       80.476949\n",
      "std       105.546355\n",
      "min         0.000012\n",
      "25%        15.003056\n",
      "50%        39.230706\n",
      "75%        99.385810\n",
      "max       742.693426\n",
      "Name: ttf_days, dtype: float64\n",
      "[INFO] Trainable rows (uncensored): 2833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/27277503.py:39: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Final training table shape: (2833, 60)\n",
      "[INFO] Split sizes: 1990 422 421\n",
      "\n",
      "[BASELINE] Last-interval mean (k=3)\n",
      "MAE:   41.49 days\n",
      "MedAE: 23.40 days\n",
      "RMSE:  65.82 days\n",
      "\n",
      "[TRAIN] Fitting RandomForestRegressor with imputers...\n",
      "[TRAIN] Done.\n",
      "\n",
      "[TEST] RandomForestRegressor performance\n",
      "MAE:   53.59 days\n",
      "MedAE: 39.41 days\n",
      "RMSE:  72.80 days\n",
      "\n",
      "[EXPLAIN] Permutation importances (subset)\n",
      "[WARN] Permutation importance skipped: All arrays must be of the same length\n",
      "\n",
      "[DONE] End-to-end MTBF (TTF) pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "# MTBF (Time-to-Next-Failure) end-to-end training script\n",
    "# - Uses Asset_System_ID as the asset key\n",
    "# - Builds time-to-next-failure labels with censor handling\n",
    "# - Engineers rolling/snapshot features\n",
    "# - Trains: (1) last-k baseline, (2) RandomForestRegressor (with imputers)\n",
    "# - Evaluates with MAE/MedAE/RMSE on a time-based test split\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# -----------------------\n",
    "# 0) Load\n",
    "# -----------------------\n",
    "PATH = Path(\"/kaggle/input/proserv-360-uiu-conbined-dataset-kader-and-kabir/Minimized data.xlsx\")  # change if needed\n",
    "assert PATH.exists(), f\"File not found: {PATH}\"\n",
    "\n",
    "df_raw = pd.read_excel(PATH)\n",
    "print(f\"[INFO] Loaded shape: {df_raw.shape}\")\n",
    "print(\"[INFO] Columns (first 25):\", list(df_raw.columns)[:25])\n",
    "\n",
    "# -----------------------\n",
    "# 1) Column utilities & detection\n",
    "# -----------------------\n",
    "ASSET_COL = \"Asset_System_ID\"\n",
    "assert ASSET_COL in df_raw.columns, f\"Expected asset key '{ASSET_COL}' not found.\"\n",
    "\n",
    "def coerce_datetime(s):\n",
    "    try:\n",
    "        return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "    except Exception:\n",
    "        return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def find_time_col(df):\n",
    "    priority = [\n",
    "        \"SR_Open_Date\", \"SR_Open_Time\", \"SR_Start_Time\", \"SR_Create_Date\", \"SR_Opened_Date\",\n",
    "        \"SR_Date\", \"SR_Open_Timestamp\", \"SR_Created_On\", \"SR_Creation_Time\", \"SR_Start_Date\",\n",
    "        \"SR_Close_Date\", \"SR_Closed_Date\", \"SR_End_Time\"\n",
    "    ]\n",
    "    for c in priority:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    candidates = [c for c in df.columns if re.search(r\"(date|time|timestamp)\", c, re.IGNORECASE)]\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "TIME_CAND = find_time_col(df)\n",
    "if TIME_CAND is not None:\n",
    "    df[\"_event_time\"] = coerce_datetime(df[TIME_CAND])\n",
    "    print(f\"[INFO] Using timestamp column: {TIME_CAND}\")\n",
    "else:\n",
    "    df[\"_event_time\"] = pd.NaT\n",
    "    print(\"[WARN] No clear timestamp column; will synthesize ordering per asset.\")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Failure flag (taxonomy + simple heuristics)\n",
    "# -----------------------\n",
    "def infer_failure_flag(frame):\n",
    "    flag = pd.Series(False, index=frame.index)\n",
    "    if \"SR_Type\" in frame.columns:\n",
    "        flag = flag | frame[\"SR_Type\"].astype(str).str.contains(r\"(corrective|breakdown)\", case=False, regex=True)\n",
    "    if \"Activity_type_STD_AGG\" in frame.columns:\n",
    "        flag = flag | frame[\"Activity_type_STD_AGG\"].astype(str).str.contains(r\"corrective\", case=False, regex=True)\n",
    "\n",
    "    if \"SR_TTR\" in frame.columns:\n",
    "        not_pm = pd.Series(True, index=frame.index)\n",
    "        text_cols = [c for c in [\"SR_Type\", \"Activity_type_STD_AGG\"] if c in frame.columns]\n",
    "        if text_cols:\n",
    "            text_join = frame[text_cols].astype(str).agg(\" \".join, axis=1)\n",
    "            not_pm = ~text_join.str.contains(r\"(pm|preventive|installation|training|upgrade)\", case=False, regex=True)\n",
    "        flag = flag | ((pd.to_numeric(frame[\"SR_TTR\"], errors=\"coerce\") > 0) & not_pm.fillna(True))\n",
    "    return flag.fillna(False)\n",
    "\n",
    "df[\"is_failure\"] = infer_failure_flag(df)\n",
    "\n",
    "# -----------------------\n",
    "# 3) Ensure per-asset ordering/time\n",
    "# -----------------------\n",
    "def synthesize_time_within_asset(g):\n",
    "    g = g.copy().sort_values(\"_event_time\")\n",
    "    if g[\"_event_time\"].notna().any():\n",
    "        # Fill remaining NaTs, guarantee monotonicity\n",
    "        filled = g[\"_event_time\"].ffill().bfill()\n",
    "        order = np.arange(len(g))\n",
    "        g[\"_event_time\"] = filled + pd.to_timedelta(order, unit=\"s\")\n",
    "        return g\n",
    "    # All NaT → synthesize daily increments from either install or a fixed origin\n",
    "    origin = pd.Timestamp(\"2015-01-01\")\n",
    "    if \"Asset_Install_Date\" in g.columns:\n",
    "        inst = coerce_datetime(g[\"Asset_Install_Date\"])\n",
    "        if inst.notna().any():\n",
    "            origin = inst.min()\n",
    "    g = g.reset_index(drop=True)\n",
    "    g[\"_event_time\"] = origin + pd.to_timedelta(np.arange(len(g)), unit=\"D\")\n",
    "    return g\n",
    "\n",
    "df = df.groupby(ASSET_COL, group_keys=False).apply(synthesize_time_within_asset)\n",
    "\n",
    "# -----------------------\n",
    "# 4) Labels: time-to-next-failure (days) + censoring\n",
    "# -----------------------\n",
    "def build_next_failure_labels(g):\n",
    "    g = g.sort_values(\"_event_time\").copy()\n",
    "    fail_idx = g.index[g[\"is_failure\"]].tolist()\n",
    "    t_next = []\n",
    "    for idx, row in g.iterrows():\n",
    "        t_row = row[\"_event_time\"]\n",
    "        next_times = [g.loc[j, \"_event_time\"] for j in fail_idx if g.loc[j, \"_event_time\"] > t_row]\n",
    "        t_next.append(min(next_times) if next_times else pd.NaT)\n",
    "    g[\"t_next_failure\"] = t_next\n",
    "    g[\"ttf_days\"] = (g[\"t_next_failure\"] - g[\"_event_time\"]).dt.total_seconds() / 86400.0\n",
    "    g[\"censored\"] = g[\"t_next_failure\"].isna()\n",
    "    return g\n",
    "\n",
    "labeled = df.groupby(ASSET_COL, group_keys=False).apply(build_next_failure_labels)\n",
    "print(\"[INFO] Label stats (incl. censored):\")\n",
    "print(labeled[\"ttf_days\"].describe())\n",
    "\n",
    "# Keep uncensored (simple regression target)\n",
    "trainable = labeled[~labeled[\"censored\"] & (labeled[\"ttf_days\"] > 0)].copy()\n",
    "print(f\"[INFO] Trainable rows (uncensored): {len(trainable)}\")\n",
    "\n",
    "# -----------------------\n",
    "# 5) Rolling features/snapshots\n",
    "# -----------------------\n",
    "def add_rolling_features(frame, windows_days=(30, 90, 180)):\n",
    "    f = frame.sort_values([ASSET_COL, \"_event_time\"]).set_index(\"_event_time\").copy()\n",
    "    f[\"SR_TTR_num\"] = pd.to_numeric(f.get(\"SR_TTR\", np.nan), errors=\"coerce\")\n",
    "\n",
    "    # time since last failure (days) — robust implementation\n",
    "    def time_since_last_failure(g):\n",
    "        t_series = pd.Series(g.index, index=g.index)\n",
    "        last_fail_time = t_series.where(g[\"is_failure\"]).ffill()\n",
    "        return (t_series - last_fail_time).dt.total_seconds() / 86400.0\n",
    "    f[\"time_since_last_failure\"] = f.groupby(ASSET_COL, group_keys=False).apply(time_since_last_failure)\n",
    "\n",
    "    for W in windows_days:\n",
    "        win = f\"{W}D\"\n",
    "        key = f\"win{W}\"\n",
    "        f[f\"fail_count_{key}\"] = f[\"is_failure\"].groupby(f[ASSET_COL]).rolling(win).sum().reset_index(level=0, drop=True)\n",
    "        f[f\"mean_ttr_{key}\"] = f[\"SR_TTR_num\"].groupby(f[ASSET_COL]).rolling(win).mean().reset_index(level=0, drop=True)\n",
    "        f[f\"var_ttr_{key}\"]   = f[\"SR_TTR_num\"].groupby(f[ASSET_COL]).rolling(win).var().reset_index(level=0, drop=True)\n",
    "        f[f\"sr_count_{key}\"]  = pd.Series(1, index=f.index).groupby(f[ASSET_COL]).rolling(win).sum().reset_index(level=0, drop=True)\n",
    "\n",
    "    f = f.reset_index()\n",
    "\n",
    "    if \"Asset_Install_Date\" in f.columns:\n",
    "        inst = coerce_datetime(f[\"Asset_Install_Date\"])\n",
    "        f[\"asset_age_days\"] = (f[\"_event_time\"] - inst).dt.total_seconds() / 86400.0\n",
    "    else:\n",
    "        f[\"asset_age_days\"] = np.nan\n",
    "\n",
    "    return f\n",
    "\n",
    "feat = add_rolling_features(labeled)\n",
    "\n",
    "# Merge features with uncensored labels\n",
    "merged = feat.merge(trainable[[ASSET_COL, \"_event_time\", \"ttf_days\"]],\n",
    "                    on=[ASSET_COL, \"_event_time\"], how=\"inner\")\n",
    "\n",
    "# If duplicate ttf columns appear, coalesce to a single 'ttf_days'\n",
    "if \"ttf_days\" not in merged.columns:\n",
    "    if \"ttf_days_x\" in merged.columns or \"ttf_days_y\" in merged.columns:\n",
    "        merged[\"ttf_days\"] = merged.get(\"ttf_days_x\", np.nan).fillna(merged.get(\"ttf_days_y\", np.nan))\n",
    "        merged = merged.drop(columns=[c for c in [\"ttf_days_x\", \"ttf_days_y\"] if c in merged.columns])\n",
    "\n",
    "train_df = merged\n",
    "print(\"[INFO] Final training table shape:\", train_df.shape)\n",
    "\n",
    "# -----------------------\n",
    "# 6) Feature lists\n",
    "# -----------------------\n",
    "categorical_cols = [c for c in [\n",
    "    \"OPH_Modality\", \"OPH_Product_Line\", \"OPH_Family_Name\",\n",
    "    \"OPH_Planning_Name\", \"OPH_Product_Group\", \"OPH_PSI_Description\",\n",
    "    \"OPH_Segment\", \"OPH_Product_Set_Desc\", \"City\", \"asset_installed_city\", \"site_name\",\n",
    "    \"SR_Type\", \"Activity_type_STD_AGG\"\n",
    "] if c in train_df.columns]\n",
    "\n",
    "numeric_cols = [c for c in train_df.columns if re.match(\n",
    "    r\"(fail_count_|mean_ttr_|var_ttr_|sr_count_|asset_age_days|time_since_last_failure)$\", c) or c in [\"SR_TTR_num\"]\n",
    "]\n",
    "\n",
    "y_col = \"ttf_days\"\n",
    "X_cols = categorical_cols + numeric_cols\n",
    "assert y_col in train_df.columns, \"ttf_days not found after merge.\"\n",
    "\n",
    "# -----------------------\n",
    "# 7) Time-based split (70/15/15 by unique event times)\n",
    "# -----------------------\n",
    "times_sorted = np.sort(train_df[\"_event_time\"].unique())\n",
    "cut1 = int(0.7 * len(times_sorted))\n",
    "cut2 = int(0.85 * len(times_sorted))\n",
    "t_train_max = times_sorted[cut1] if len(times_sorted) else pd.Timestamp(\"2022-01-01\")\n",
    "t_valid_max = times_sorted[cut2] if len(times_sorted) else pd.Timestamp(\"2023-01-01\")\n",
    "\n",
    "def split_by_time(frame):\n",
    "    tr = frame[frame[\"_event_time\"] <= t_train_max]\n",
    "    va = frame[(frame[\"_event_time\"] > t_train_max) & (frame[\"_event_time\"] <= t_valid_max)]\n",
    "    te = frame[frame[\"_event_time\"] > t_valid_max]\n",
    "    return tr, va, te\n",
    "\n",
    "train_split, valid_split, test_split = split_by_time(train_df)\n",
    "print(\"[INFO] Split sizes:\", len(train_split), len(valid_split), len(test_split))\n",
    "\n",
    "# -----------------------\n",
    "# 8) Baseline: last-k (k=3) mean of prior inter-failure gaps per asset\n",
    "# -----------------------\n",
    "def compute_last_k_baseline(df_all, k=3):\n",
    "    hist = df_all.sort_values([ASSET_COL, \"_event_time\"]).copy()\n",
    "    ttf = hist.groupby(ASSET_COL)[\"ttf_days\"].apply(lambda s: s.shift().rolling(k, min_periods=1).mean())\n",
    "    hist[\"asset_ttf_hist\"] = ttf.reset_index(level=0, drop=True)\n",
    "    return hist[\"asset_ttf_hist\"]\n",
    "\n",
    "baseline_all = pd.concat([train_split, valid_split, test_split], axis=0).sort_values([ASSET_COL, \"_event_time\"])\n",
    "baseline_all[\"baseline_pred\"] = compute_last_k_baseline(baseline_all, k=3)\n",
    "\n",
    "global_mean = train_split[y_col].mean() if len(train_split) else train_df[y_col].mean()\n",
    "test_baseline = baseline_all.loc[test_split.index, \"baseline_pred\"].fillna(global_mean)\n",
    "\n",
    "baseline_mae   = mean_absolute_error(test_split[y_col], test_baseline) if len(test_split) else np.nan\n",
    "baseline_medae = median_absolute_error(test_split[y_col], test_baseline) if len(test_split) else np.nan\n",
    "baseline_rmse  = mean_squared_error(test_split[y_col], test_baseline, squared=False) if len(test_split) else np.nan\n",
    "\n",
    "print(\"\\n[BASELINE] Last-interval mean (k=3)\")\n",
    "print(f\"MAE:   {baseline_mae:,.2f} days\" if pd.notna(baseline_mae) else \"MAE: N/A\")\n",
    "print(f\"MedAE: {baseline_medae:,.2f} days\" if pd.notna(baseline_medae) else \"MedAE: N/A\")\n",
    "print(f\"RMSE:  {baseline_rmse:,.2f} days\" if pd.notna(baseline_rmse) else \"RMSE: N/A\")\n",
    "\n",
    "# -----------------------\n",
    "# 9) Model: RandomForestRegressor with imputers + preprocessing\n",
    "# -----------------------\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=10)),\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler()),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[(\"prep\", preprocess), (\"rf\", rf)])\n",
    "\n",
    "X_train = train_split[X_cols].copy()\n",
    "y_train = train_split[y_col].values\n",
    "X_valid = valid_split[X_cols].copy()\n",
    "y_valid = valid_split[y_col].values\n",
    "X_test  = test_split[X_cols].copy()\n",
    "y_test  = test_split[y_col].values\n",
    "\n",
    "print(\"\\n[TRAIN] Fitting RandomForestRegressor with imputers...\")\n",
    "model.fit(pd.concat([X_train, X_valid]), np.concatenate([y_train, y_valid]))\n",
    "print(\"[TRAIN] Done.\")\n",
    "\n",
    "pred_test = model.predict(X_test) if len(X_test) else np.array([])\n",
    "\n",
    "if len(pred_test):\n",
    "    mae   = mean_absolute_error(y_test, pred_test)\n",
    "    medae = median_absolute_error(y_test, pred_test)\n",
    "    rmse  = mean_squared_error(y_test, pred_test, squared=False)\n",
    "\n",
    "    print(\"\\n[TEST] RandomForestRegressor performance\")\n",
    "    print(f\"MAE:   {mae:,.2f} days\")\n",
    "    print(f\"MedAE: {medae:,.2f} days\")\n",
    "    print(f\"RMSE:  {rmse:,.2f} days\")\n",
    "\n",
    "    # Optional: permutation importances (subset for speed)\n",
    "    try:\n",
    "        subset_n   = min(500, len(X_test))\n",
    "        subset_idx = np.random.RandomState(42).choice(len(X_test), size=subset_n, replace=False)\n",
    "        print(\"\\n[EXPLAIN] Permutation importances (subset)\")\n",
    "        perm = permutation_importance(model, X_test.iloc[subset_idx], y_test[subset_idx],\n",
    "                                      n_repeats=3, random_state=42, n_jobs=-1)\n",
    "\n",
    "        # Reconstruct feature names\n",
    "        cat_names = []\n",
    "        if categorical_cols:\n",
    "            cat_names = list(model.named_steps[\"prep\"]\n",
    "                                   .named_transformers_[\"cat\"]\n",
    "                                   .named_steps[\"ohe\"]\n",
    "                                   .get_feature_names_out(categorical_cols))\n",
    "        num_names = numeric_cols\n",
    "        all_feat_names = cat_names + num_names\n",
    "\n",
    "        imp_df = pd.DataFrame({\n",
    "            \"feature\": all_feat_names,\n",
    "            \"importance\": perm.importances_mean\n",
    "        }).sort_values(\"importance\", ascending=False).head(20)\n",
    "\n",
    "        print(\"\\nTop 20 features by permutation importance:\")\n",
    "        for i, (f, v) in enumerate(zip(imp_df[\"feature\"], imp_df[\"importance\"]), 1):\n",
    "            print(f\"{i:>2}. {f:50s}  {v: .6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Permutation importance skipped: {e}\")\n",
    "else:\n",
    "    print(\"\\n[TEST] Not enough test rows to evaluate the model.\")\n",
    "\n",
    "print(\"\\n[DONE] End-to-end MTBF (TTF) pipeline complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71c975e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T18:25:55.550899Z",
     "iopub.status.busy": "2025-10-22T18:25:55.550580Z",
     "iopub.status.idle": "2025-10-22T18:25:57.943670Z",
     "shell.execute_reply": "2025-10-22T18:25:57.942404Z"
    },
    "papermill": {
     "duration": 2.400471,
     "end_time": "2025-10-22T18:25:57.945334",
     "exception": false,
     "start_time": "2025-10-22T18:25:55.544863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/2199906926.py:37: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_13/2199906926.py:103: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(ASSET_COL, group_keys=False).apply(per_asset)\n",
      "/tmp/ipykernel_13/2199906926.py:37: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_13/2199906926.py:103: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(ASSET_COL, group_keys=False).apply(per_asset)\n",
      "/tmp/ipykernel_13/2199906926.py:60: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  flag = flag | df[\"SR_Type\"].astype(str).str.contains(FAILURE_REGEX, case=False, regex=True)\n",
      "/tmp/ipykernel_13/2199906926.py:68: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  not_pm = ~text_join.str.contains(NON_FAILURE_REGEX, case=False, regex=True)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n",
      "  return op(a, b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MODEL] Learned statistics\n",
      "  Global MTBF mean (days): 63.24\n",
      "  Assets with history (n_samples>=1): 148\n",
      "  Cohort table: OPH_Product_Line (groups=5)\n",
      "  Cohort table: OPH_Modality (groups=5)\n",
      "  Cohort table: OPH_Family_Name (groups=28)\n",
      "\n",
      "[PREDICTIONS] MTBF days by System ID (per-asset if available):\n",
      "Asset_System_ID  n_samples  asset_mean  asset_ema  asset_last3  cohort_mean  global_mean  prediction_days         strategy\n",
      " 83021600005521          6   91.498127 128.780676   104.698549    82.933964    63.239357        91.498127 asset_mean(n>=1)\n",
      " 83021600015312          6  133.976250 136.814831   125.957315    82.933964    63.239357       133.976250 asset_mean(n>=1)\n",
      " 83021600098222         12   45.519664  49.761622    48.659726    82.933964    63.239357        45.519664 asset_mean(n>=1)\n",
      " 83021600100022          4  155.065191  81.534463   204.000841    82.933964    63.239357       155.065191 asset_mean(n>=1)\n",
      " 83021600103323         14   29.446710  30.349376    35.096377    82.933964    63.239357        29.446710 asset_mean(n>=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/2199906926.py:122: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(ASSET_COL, group_keys=False).apply(asset_gaps)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MTBF-by-SystemID Model (Corrected)\n",
    "- X: Asset_System_ID (string-normalized)\n",
    "- Y: MTBF (expected inter-failure interval), learned from historical gaps\n",
    "- Strategy:\n",
    "    * If an asset has >=1 MTBF samples -> use per-asset MEAN (deterministic, per-ID)\n",
    "    * Else fall back to cohort mean (Product Line / Modality / Family) -> else global mean\n",
    "- Fixes:\n",
    "    * Normalize Asset_System_ID to str in both fit & predict to avoid fallback for dtype mismatch\n",
    "    * Compute EMA/last-K in true time order (using _event_time)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "EXCEL_PATH = Path(\"/kaggle/input/proserv-360-uiu-conbined-dataset-kader-and-kabir/Minimized data.xlsx\")  # change if needed\n",
    "ASSET_COL   = \"Asset_System_ID\"\n",
    "\n",
    "# Cohort columns (ordered fallback preference)\n",
    "COHORT_COLS = [\"OPH_Product_Line\", \"OPH_Modality\", \"OPH_Family_Name\"]\n",
    "\n",
    "# Failure taxonomy (adjust if you have a clean SR_Type list)\n",
    "FAILURE_REGEX     = r\"(corrective|breakdown)\"\n",
    "NON_FAILURE_REGEX = r\"(pm|preventive|installation|training|upgrade)\"  # to exclude non-failures in heuristic\n",
    "\n",
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def coerce_datetime(s: pd.Series) -> pd.Series:\n",
    "    try:\n",
    "        return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "    except Exception:\n",
    "        return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def find_time_col(df: pd.DataFrame) -> Optional[str]:\n",
    "    priority = [\n",
    "        \"SR_Open_Date\", \"SR_Open_Time\", \"SR_Start_Time\", \"SR_Create_Date\", \"SR_Opened_Date\",\n",
    "        \"SR_Date\", \"SR_Open_Timestamp\", \"SR_Created_On\", \"SR_Creation_Time\", \"SR_Start_Date\",\n",
    "        \"SR_Close_Date\", \"SR_Closed_Date\", \"SR_End_Time\",\n",
    "    ]\n",
    "    for c in priority:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # fallback: first date/time-like column\n",
    "    for c in df.columns:\n",
    "        if re.search(r\"(date|time|timestamp)\", str(c), re.IGNORECASE):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def infer_failure_flag(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Mark service-impacting failures (heuristics + declared types).\"\"\"\n",
    "    flag = pd.Series(False, index=df.index)\n",
    "    if \"SR_Type\" in df.columns:\n",
    "        flag = flag | df[\"SR_Type\"].astype(str).str.contains(FAILURE_REGEX, case=False, regex=True)\n",
    "    if \"Activity_type_STD_AGG\" in df.columns:\n",
    "        flag = flag | df[\"Activity_type_STD_AGG\"].astype(str).str.contains(\"corrective\", case=False, regex=True)\n",
    "\n",
    "    if \"SR_TTR\" in df.columns:\n",
    "        text_cols = [c for c in [\"SR_Type\", \"Activity_type_STD_AGG\"] if c in df.columns]\n",
    "        if text_cols:\n",
    "            text_join = df[text_cols].astype(str).agg(\" \".join, axis=1)\n",
    "            not_pm = ~text_join.str.contains(NON_FAILURE_REGEX, case=False, regex=True)\n",
    "        else:\n",
    "            not_pm = True\n",
    "        flag = flag | ((pd.to_numeric(df[\"SR_TTR\"], errors=\"coerce\") > 0) & pd.Series(not_pm, index=df.index))\n",
    "    return flag.fillna(False)\n",
    "\n",
    "def ensure_event_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure each asset has a monotone event time to order events, and normalize System ID to str.\"\"\"\n",
    "    df = df.copy()\n",
    "    # normalize System ID to str early\n",
    "    df[ASSET_COL] = df[ASSET_COL].astype(str)\n",
    "\n",
    "    tcol = find_time_col(df)\n",
    "    if tcol:\n",
    "        df[\"_event_time\"] = coerce_datetime(df[tcol])\n",
    "    else:\n",
    "        df[\"_event_time\"] = pd.NaT\n",
    "\n",
    "    def per_asset(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = g.sort_values(\"_event_time\").copy()\n",
    "        if g[\"_event_time\"].notna().any():\n",
    "            filled = g[\"_event_time\"].ffill().bfill()\n",
    "            order  = np.arange(len(g))\n",
    "            g[\"_event_time\"] = filled + pd.to_timedelta(order, unit=\"s\")\n",
    "            return g\n",
    "        # all NaT => synthesize from install date or a fixed origin\n",
    "        origin = pd.Timestamp(\"2015-01-01\")\n",
    "        if \"Asset_Install_Date\" in g.columns:\n",
    "            inst = coerce_datetime(g[\"Asset_Install_Date\"])\n",
    "            if inst.notna().any():\n",
    "                origin = inst.min()\n",
    "        g = g.reset_index(drop=True)\n",
    "        g[\"_event_time\"] = origin + pd.to_timedelta(np.arange(len(g)), unit=\"D\")\n",
    "        return g\n",
    "\n",
    "    return df.groupby(ASSET_COL, group_keys=False).apply(per_asset)\n",
    "\n",
    "def build_inter_failure_intervals(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute inter-failure intervals (days) for each asset from consecutive failure events.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"is_failure\"] = infer_failure_flag(df)\n",
    "    df = df.sort_values([ASSET_COL, \"_event_time\"])\n",
    "\n",
    "    def asset_gaps(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = g.sort_values(\"_event_time\").copy()\n",
    "        fail_times = g.loc[g[\"is_failure\"], \"_event_time\"]\n",
    "        if len(fail_times) < 2:\n",
    "            g[\"mtbf_sample_days\"] = np.nan\n",
    "            return g\n",
    "        gaps = (fail_times.shift(-1) - fail_times).dt.total_seconds() / 86400.0\n",
    "        g[\"mtbf_sample_days\"] = np.nan\n",
    "        g.loc[fail_times.index[:-1], \"mtbf_sample_days\"] = gaps.iloc[:-1].values\n",
    "        return g\n",
    "\n",
    "    return df.groupby(ASSET_COL, group_keys=False).apply(asset_gaps)\n",
    "\n",
    "def exp_moving_average(sequence: pd.Series, alpha: float = 0.5) -> float:\n",
    "    vals = pd.to_numeric(sequence, errors=\"coerce\").dropna().values\n",
    "    if len(vals) == 0:\n",
    "        return np.nan\n",
    "    ema = vals[0]\n",
    "    for v in vals[1:]:\n",
    "        ema = alpha * v + (1 - alpha) * ema\n",
    "    return float(ema)\n",
    "\n",
    "# -----------------------\n",
    "# Model\n",
    "# -----------------------\n",
    "class MTBFModel:\n",
    "    \"\"\"\n",
    "    Lookup-style MTBF model per System ID (string-normalized).\n",
    "      - If an asset has >=1 MTBF samples -> use per-asset mean (stable per-ID output)\n",
    "      - Else -> cohort mean (Product Line / Modality / Family)\n",
    "      - Else -> global mean\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cohort_cols: List[str] = COHORT_COLS):\n",
    "        self.cohort_cols = list(cohort_cols)\n",
    "        self.asset_stats: pd.DataFrame = pd.DataFrame()\n",
    "        self.cohort_stats: Dict[str, pd.DataFrame] = {}\n",
    "        self.global_mean: float = float(\"nan\")\n",
    "\n",
    "    def fit(self, df_raw: pd.DataFrame) -> \"MTBFModel\":\n",
    "        assert ASSET_COL in df_raw.columns, f\"Missing {ASSET_COL}\"\n",
    "\n",
    "        # Prepare & normalize types\n",
    "        df = ensure_event_time(df_raw)\n",
    "        df = build_inter_failure_intervals(df)\n",
    "\n",
    "        # Collect MTBF samples with time & cohorts\n",
    "        keep_cols = [ASSET_COL, \"_event_time\", \"mtbf_sample_days\"] + [c for c in self.cohort_cols if c in df.columns]\n",
    "        samples = df.loc[~df[\"mtbf_sample_days\"].isna(), keep_cols].copy()\n",
    "\n",
    "        # Normalize System ID to string (in case ensure_event_time missed anything upstream)\n",
    "        samples[ASSET_COL] = samples[ASSET_COL].astype(str)\n",
    "\n",
    "        # Per-asset aggregates\n",
    "        agg = (samples.groupby(ASSET_COL)[\"mtbf_sample_days\"]\n",
    "                      .agg(n_samples=\"count\", asset_mean=\"mean\", asset_median=\"median\"))\n",
    "\n",
    "        # EMA & last-3 in true time order\n",
    "        ema_vals = []\n",
    "        last3_vals = []\n",
    "        for asset_id, g in samples.groupby(ASSET_COL):\n",
    "            seq = g.sort_values(\"_event_time\")[\"mtbf_sample_days\"]\n",
    "            ema_vals.append(exp_moving_average(seq, alpha=0.5))\n",
    "            last3_vals.append(float(np.nanmean(seq.tail(3))) if len(seq) else np.nan)\n",
    "\n",
    "        agg[\"asset_ema\"]   = ema_vals\n",
    "        agg[\"asset_last3\"] = last3_vals\n",
    "\n",
    "        # Ensure string index\n",
    "        agg.index = agg.index.astype(str)\n",
    "        self.asset_stats = agg\n",
    "\n",
    "        # Cohort stats\n",
    "        for col in self.cohort_cols:\n",
    "            if col in samples.columns:\n",
    "                tab = (samples.groupby(col)[\"mtbf_sample_days\"]\n",
    "                             .agg(n=\"count\", mean=\"mean\", median=\"median\"))\n",
    "                self.cohort_stats[col] = tab\n",
    "\n",
    "        # Global mean\n",
    "        self.global_mean = float(samples[\"mtbf_sample_days\"].mean()) if len(samples) else float(\"nan\")\n",
    "        return self\n",
    "\n",
    "    def _lookup_cohort_mean(self, df_row: pd.Series) -> Optional[float]:\n",
    "        for col in self.cohort_cols:\n",
    "            if col in df_row.index and col in self.cohort_stats and pd.notna(df_row[col]):\n",
    "                ctab = self.cohort_stats[col]\n",
    "                key = df_row[col]\n",
    "                if key in ctab.index and pd.notna(ctab.loc[key, \"mean\"]):\n",
    "                    return float(ctab.loc[key, \"mean\"])\n",
    "        return None\n",
    "\n",
    "    def predict_single(self, system_id: Any, df_context: Optional[pd.DataFrame] = None) -> Dict[str, Any]:\n",
    "        sid = str(system_id)  # normalize\n",
    "        out = {\n",
    "            ASSET_COL: sid,\n",
    "            \"n_samples\": 0,\n",
    "            \"asset_mean\": None,\n",
    "            \"asset_ema\": None,\n",
    "            \"asset_last3\": None,\n",
    "            \"cohort_mean\": None,\n",
    "            \"global_mean\": self.global_mean,\n",
    "            \"prediction_days\": None,\n",
    "            \"strategy\": None,\n",
    "        }\n",
    "\n",
    "        # Per-asset stats (string index)\n",
    "        if sid in self.asset_stats.index:\n",
    "            row = self.asset_stats.loc[sid]\n",
    "            out[\"n_samples\"]  = int(row[\"n_samples\"])\n",
    "            out[\"asset_mean\"] = (None if pd.isna(row[\"asset_mean\"]) else float(row[\"asset_mean\"]))\n",
    "            out[\"asset_ema\"]  = (None if pd.isna(row[\"asset_ema\"]) else float(row[\"asset_ema\"]))\n",
    "            out[\"asset_last3\"]= (None if pd.isna(row[\"asset_last3\"]) else float(row[\"asset_last3\"]))\n",
    "\n",
    "        # Cohort fallback needs asset attributes\n",
    "        if df_context is not None and out[\"cohort_mean\"] is None:\n",
    "            ctx = df_context.copy()\n",
    "            ctx[ASSET_COL] = ctx[ASSET_COL].astype(str)\n",
    "            rows = ctx.loc[ctx[ASSET_COL] == sid]\n",
    "            if len(rows):\n",
    "                latest = rows.sort_values(\"_event_time\").iloc[-1]\n",
    "                cmean = self._lookup_cohort_mean(latest)\n",
    "                if cmean is not None:\n",
    "                    out[\"cohort_mean\"] = cmean\n",
    "\n",
    "        # Prediction policy:\n",
    "        # Prefer per-asset MEAN if any history exists (ensures per-ID differences)\n",
    "        if out[\"n_samples\"] >= 1 and out[\"asset_mean\"] is not None:\n",
    "            out[\"prediction_days\"] = out[\"asset_mean\"]\n",
    "            out[\"strategy\"] = \"asset_mean(n>=1)\"\n",
    "        elif out[\"cohort_mean\"] is not None:\n",
    "            out[\"prediction_days\"] = float(out[\"cohort_mean\"])\n",
    "            out[\"strategy\"] = \"cohort_mean\"\n",
    "        else:\n",
    "            out[\"prediction_days\"] = float(out[\"global_mean\"])\n",
    "            out[\"strategy\"] = \"global_mean\"\n",
    "\n",
    "        return out\n",
    "\n",
    "    def predict(self, system_ids: List[Any], df_context: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        rows = [self.predict_single(sid, df_context=df_context) for sid in system_ids]\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "# -----------------------\n",
    "# Train the model and demo predictions\n",
    "# -----------------------\n",
    "def main():\n",
    "    assert EXCEL_PATH.exists(), f\"File not found: {EXCEL_PATH}\"\n",
    "    df = pd.read_excel(EXCEL_PATH)\n",
    "\n",
    "    # Prepare timeline & normalize IDs (adds _event_time)\n",
    "    df = ensure_event_time(df)\n",
    "\n",
    "    # Fit\n",
    "    model = MTBFModel(cohort_cols=COHORT_COLS).fit(df)\n",
    "\n",
    "    print(\"\\n[MODEL] Learned statistics\")\n",
    "    print(f\"  Global MTBF mean (days): {model.global_mean:.2f}\" if not np.isnan(model.global_mean) else \"  Global MTBF mean: NaN\")\n",
    "    print(f\"  Assets with history (n_samples>=1): {int((model.asset_stats['n_samples'] >= 1).sum())}\")\n",
    "\n",
    "    for c in COHORT_COLS:\n",
    "        if c in model.cohort_stats:\n",
    "            print(f\"  Cohort table: {c} (groups={len(model.cohort_stats[c])})\")\n",
    "\n",
    "    # Demo predictions for first 5 IDs in the data\n",
    "    demo_ids = list(df[ASSET_COL].astype(str).dropna().unique()[:5])\n",
    "    preds = model.predict(demo_ids, df_context=df)\n",
    "\n",
    "    print(\"\\n[PREDICTIONS] MTBF days by System ID (per-asset if available):\")\n",
    "    print(preds.to_string(index=False))\n",
    "\n",
    "    # Example callable:\n",
    "    # def predict_mtbf(system_id: str) -> float:\n",
    "    #     return float(model.predict([system_id], df_context=df).iloc[0][\"prediction_days\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d05bdc89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T18:25:57.956840Z",
     "iopub.status.busy": "2025-10-22T18:25:57.956486Z",
     "iopub.status.idle": "2025-10-22T18:26:00.535890Z",
     "shell.execute_reply": "2025-10-22T18:26:00.534540Z"
    },
    "papermill": {
     "duration": 2.58731,
     "end_time": "2025-10-22T18:26:00.537757",
     "exception": false,
     "start_time": "2025-10-22T18:25:57.950447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/2980404613.py:37: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_13/2980404613.py:103: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(ASSET_COL, group_keys=False).apply(per_asset)\n",
      "/tmp/ipykernel_13/2980404613.py:37: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_13/2980404613.py:103: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(ASSET_COL, group_keys=False).apply(per_asset)\n",
      "/tmp/ipykernel_13/2980404613.py:60: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  flag = flag | df[\"SR_Type\"].astype(str).str.contains(FAILURE_REGEX, case=False, regex=True)\n",
      "/tmp/ipykernel_13/2980404613.py:68: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  not_pm = ~text_join.str.contains(NON_FAILURE_REGEX, case=False, regex=True)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n",
      "  return op(a, b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MODEL] Learned statistics\n",
      "  Global MTBF mean (days): 63.24\n",
      "  Assets with history (n_samples>=1): 148\n",
      "  Cohort table: OPH_Product_Line (groups=5)\n",
      "  Cohort table: OPH_Modality (groups=5)\n",
      "  Cohort table: OPH_Family_Name (groups=28)\n",
      "\n",
      "[PREDICTIONS] MTBF days by System ID (per-asset if available):\n",
      "Asset_System_ID  n_samples  asset_mean  asset_ema  asset_last3  cohort_mean  global_mean  prediction_days         strategy\n",
      " 83021600005521          6   91.498127 128.780676   104.698549    82.933964    63.239357        91.498127 asset_mean(n>=1)\n",
      " 83021600015312          6  133.976250 136.814831   125.957315    82.933964    63.239357       133.976250 asset_mean(n>=1)\n",
      " 83021600098222         12   45.519664  49.761622    48.659726    82.933964    63.239357        45.519664 asset_mean(n>=1)\n",
      " 83021600100022          4  155.065191  81.534463   204.000841    82.933964    63.239357       155.065191 asset_mean(n>=1)\n",
      " 83021600103323         14   29.446710  30.349376    35.096377    82.933964    63.239357        29.446710 asset_mean(n>=1)\n",
      " 83021600108822          8  104.887859  71.702315    65.888750    82.933964    63.239357       104.887859 asset_mean(n>=1)\n",
      " 83021600108923         10   28.924380  33.784702    25.025895    82.933964    63.239357        28.924380 asset_mean(n>=1)\n",
      " 83021600184824          5   64.219759  77.730433   102.283742    82.933964    63.239357        64.219759 asset_mean(n>=1)\n",
      " 83021600420416         27   35.893426  23.527434    17.434460    82.933964    63.239357        35.893426 asset_mean(n>=1)\n",
      " 83021600430123          4  142.239965 123.837844    75.999514    82.933964    63.239357       142.239965 asset_mean(n>=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/2980404613.py:122: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(ASSET_COL, group_keys=False).apply(asset_gaps)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MTBF-by-SystemID Model (Corrected)\n",
    "- X: Asset_System_ID (string-normalized)\n",
    "- Y: MTBF (expected inter-failure interval), learned from historical gaps\n",
    "- Strategy:\n",
    "    * If an asset has >=1 MTBF samples -> use per-asset MEAN (deterministic, per-ID)\n",
    "    * Else fall back to cohort mean (Product Line / Modality / Family) -> else global mean\n",
    "- Fixes:\n",
    "    * Normalize Asset_System_ID to str in both fit & predict to avoid fallback for dtype mismatch\n",
    "    * Compute EMA/last-K in true time order (using _event_time)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "EXCEL_PATH = Path(\"/kaggle/input/proserv-360-uiu-conbined-dataset-kader-and-kabir/Minimized data.xlsx\")  # change if needed\n",
    "ASSET_COL   = \"Asset_System_ID\"\n",
    "\n",
    "# Cohort columns (ordered fallback preference)\n",
    "COHORT_COLS = [\"OPH_Product_Line\", \"OPH_Modality\", \"OPH_Family_Name\"]\n",
    "\n",
    "# Failure taxonomy (adjust if you have a clean SR_Type list)\n",
    "FAILURE_REGEX     = r\"(corrective|breakdown)\"\n",
    "NON_FAILURE_REGEX = r\"(pm|preventive|installation|training|upgrade)\"  # to exclude non-failures in heuristic\n",
    "\n",
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def coerce_datetime(s: pd.Series) -> pd.Series:\n",
    "    try:\n",
    "        return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "    except Exception:\n",
    "        return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def find_time_col(df: pd.DataFrame) -> Optional[str]:\n",
    "    priority = [\n",
    "        \"SR_Open_Date\", \"SR_Open_Time\", \"SR_Start_Time\", \"SR_Create_Date\", \"SR_Opened_Date\",\n",
    "        \"SR_Date\", \"SR_Open_Timestamp\", \"SR_Created_On\", \"SR_Creation_Time\", \"SR_Start_Date\",\n",
    "        \"SR_Close_Date\", \"SR_Closed_Date\", \"SR_End_Time\",\n",
    "    ]\n",
    "    for c in priority:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # fallback: first date/time-like column\n",
    "    for c in df.columns:\n",
    "        if re.search(r\"(date|time|timestamp)\", str(c), re.IGNORECASE):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def infer_failure_flag(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Mark service-impacting failures (heuristics + declared types).\"\"\"\n",
    "    flag = pd.Series(False, index=df.index)\n",
    "    if \"SR_Type\" in df.columns:\n",
    "        flag = flag | df[\"SR_Type\"].astype(str).str.contains(FAILURE_REGEX, case=False, regex=True)\n",
    "    if \"Activity_type_STD_AGG\" in df.columns:\n",
    "        flag = flag | df[\"Activity_type_STD_AGG\"].astype(str).str.contains(\"corrective\", case=False, regex=True)\n",
    "\n",
    "    if \"SR_TTR\" in df.columns:\n",
    "        text_cols = [c for c in [\"SR_Type\", \"Activity_type_STD_AGG\"] if c in df.columns]\n",
    "        if text_cols:\n",
    "            text_join = df[text_cols].astype(str).agg(\" \".join, axis=1)\n",
    "            not_pm = ~text_join.str.contains(NON_FAILURE_REGEX, case=False, regex=True)\n",
    "        else:\n",
    "            not_pm = True\n",
    "        flag = flag | ((pd.to_numeric(df[\"SR_TTR\"], errors=\"coerce\") > 0) & pd.Series(not_pm, index=df.index))\n",
    "    return flag.fillna(False)\n",
    "\n",
    "def ensure_event_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure each asset has a monotone event time to order events, and normalize System ID to str.\"\"\"\n",
    "    df = df.copy()\n",
    "    # normalize System ID to str early\n",
    "    df[ASSET_COL] = df[ASSET_COL].astype(str)\n",
    "\n",
    "    tcol = find_time_col(df)\n",
    "    if tcol:\n",
    "        df[\"_event_time\"] = coerce_datetime(df[tcol])\n",
    "    else:\n",
    "        df[\"_event_time\"] = pd.NaT\n",
    "\n",
    "    def per_asset(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = g.sort_values(\"_event_time\").copy()\n",
    "        if g[\"_event_time\"].notna().any():\n",
    "            filled = g[\"_event_time\"].ffill().bfill()\n",
    "            order  = np.arange(len(g))\n",
    "            g[\"_event_time\"] = filled + pd.to_timedelta(order, unit=\"s\")\n",
    "            return g\n",
    "        # all NaT => synthesize from install date or a fixed origin\n",
    "        origin = pd.Timestamp(\"2015-01-01\")\n",
    "        if \"Asset_Install_Date\" in g.columns:\n",
    "            inst = coerce_datetime(g[\"Asset_Install_Date\"])\n",
    "            if inst.notna().any():\n",
    "                origin = inst.min()\n",
    "        g = g.reset_index(drop=True)\n",
    "        g[\"_event_time\"] = origin + pd.to_timedelta(np.arange(len(g)), unit=\"D\")\n",
    "        return g\n",
    "\n",
    "    return df.groupby(ASSET_COL, group_keys=False).apply(per_asset)\n",
    "\n",
    "def build_inter_failure_intervals(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute inter-failure intervals (days) for each asset from consecutive failure events.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"is_failure\"] = infer_failure_flag(df)\n",
    "    df = df.sort_values([ASSET_COL, \"_event_time\"])\n",
    "\n",
    "    def asset_gaps(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = g.sort_values(\"_event_time\").copy()\n",
    "        fail_times = g.loc[g[\"is_failure\"], \"_event_time\"]\n",
    "        if len(fail_times) < 2:\n",
    "            g[\"mtbf_sample_days\"] = np.nan\n",
    "            return g\n",
    "        gaps = (fail_times.shift(-1) - fail_times).dt.total_seconds() / 86400.0\n",
    "        g[\"mtbf_sample_days\"] = np.nan\n",
    "        g.loc[fail_times.index[:-1], \"mtbf_sample_days\"] = gaps.iloc[:-1].values\n",
    "        return g\n",
    "\n",
    "    return df.groupby(ASSET_COL, group_keys=False).apply(asset_gaps)\n",
    "\n",
    "def exp_moving_average(sequence: pd.Series, alpha: float = 0.5) -> float:\n",
    "    vals = pd.to_numeric(sequence, errors=\"coerce\").dropna().values\n",
    "    if len(vals) == 0:\n",
    "        return np.nan\n",
    "    ema = vals[0]\n",
    "    for v in vals[1:]:\n",
    "        ema = alpha * v + (1 - alpha) * ema\n",
    "    return float(ema)\n",
    "\n",
    "# -----------------------\n",
    "# Model\n",
    "# -----------------------\n",
    "class MTBFModel:\n",
    "    \"\"\"\n",
    "    Lookup-style MTBF model per System ID (string-normalized).\n",
    "      - If an asset has >=1 MTBF samples -> use per-asset mean (stable per-ID output)\n",
    "      - Else -> cohort mean (Product Line / Modality / Family)\n",
    "      - Else -> global mean\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cohort_cols: List[str] = COHORT_COLS):\n",
    "        self.cohort_cols = list(cohort_cols)\n",
    "        self.asset_stats: pd.DataFrame = pd.DataFrame()\n",
    "        self.cohort_stats: Dict[str, pd.DataFrame] = {}\n",
    "        self.global_mean: float = float(\"nan\")\n",
    "\n",
    "    def fit(self, df_raw: pd.DataFrame) -> \"MTBFModel\":\n",
    "        assert ASSET_COL in df_raw.columns, f\"Missing {ASSET_COL}\"\n",
    "\n",
    "        # Prepare & normalize types\n",
    "        df = ensure_event_time(df_raw)\n",
    "        df = build_inter_failure_intervals(df)\n",
    "\n",
    "        # Collect MTBF samples with time & cohorts\n",
    "        keep_cols = [ASSET_COL, \"_event_time\", \"mtbf_sample_days\"] + [c for c in self.cohort_cols if c in df.columns]\n",
    "        samples = df.loc[~df[\"mtbf_sample_days\"].isna(), keep_cols].copy()\n",
    "\n",
    "        # Normalize System ID to string (in case ensure_event_time missed anything upstream)\n",
    "        samples[ASSET_COL] = samples[ASSET_COL].astype(str)\n",
    "\n",
    "        # Per-asset aggregates\n",
    "        agg = (samples.groupby(ASSET_COL)[\"mtbf_sample_days\"]\n",
    "                      .agg(n_samples=\"count\", asset_mean=\"mean\", asset_median=\"median\"))\n",
    "\n",
    "        # EMA & last-3 in true time order\n",
    "        ema_vals = []\n",
    "        last3_vals = []\n",
    "        for asset_id, g in samples.groupby(ASSET_COL):\n",
    "            seq = g.sort_values(\"_event_time\")[\"mtbf_sample_days\"]\n",
    "            ema_vals.append(exp_moving_average(seq, alpha=0.5))\n",
    "            last3_vals.append(float(np.nanmean(seq.tail(3))) if len(seq) else np.nan)\n",
    "\n",
    "        agg[\"asset_ema\"]   = ema_vals\n",
    "        agg[\"asset_last3\"] = last3_vals\n",
    "\n",
    "        # Ensure string index\n",
    "        agg.index = agg.index.astype(str)\n",
    "        self.asset_stats = agg\n",
    "\n",
    "        # Cohort stats\n",
    "        for col in self.cohort_cols:\n",
    "            if col in samples.columns:\n",
    "                tab = (samples.groupby(col)[\"mtbf_sample_days\"]\n",
    "                             .agg(n=\"count\", mean=\"mean\", median=\"median\"))\n",
    "                self.cohort_stats[col] = tab\n",
    "\n",
    "        # Global mean\n",
    "        self.global_mean = float(samples[\"mtbf_sample_days\"].mean()) if len(samples) else float(\"nan\")\n",
    "        return self\n",
    "\n",
    "    def _lookup_cohort_mean(self, df_row: pd.Series) -> Optional[float]:\n",
    "        for col in self.cohort_cols:\n",
    "            if col in df_row.index and col in self.cohort_stats and pd.notna(df_row[col]):\n",
    "                ctab = self.cohort_stats[col]\n",
    "                key = df_row[col]\n",
    "                if key in ctab.index and pd.notna(ctab.loc[key, \"mean\"]):\n",
    "                    return float(ctab.loc[key, \"mean\"])\n",
    "        return None\n",
    "\n",
    "    def predict_single(self, system_id: Any, df_context: Optional[pd.DataFrame] = None) -> Dict[str, Any]:\n",
    "        sid = str(system_id)  # normalize\n",
    "        out = {\n",
    "            ASSET_COL: sid,\n",
    "            \"n_samples\": 0,\n",
    "            \"asset_mean\": None,\n",
    "            \"asset_ema\": None,\n",
    "            \"asset_last3\": None,\n",
    "            \"cohort_mean\": None,\n",
    "            \"global_mean\": self.global_mean,\n",
    "            \"prediction_days\": None,\n",
    "            \"strategy\": None,\n",
    "        }\n",
    "\n",
    "        # Per-asset stats (string index)\n",
    "        if sid in self.asset_stats.index:\n",
    "            row = self.asset_stats.loc[sid]\n",
    "            out[\"n_samples\"]  = int(row[\"n_samples\"])\n",
    "            out[\"asset_mean\"] = (None if pd.isna(row[\"asset_mean\"]) else float(row[\"asset_mean\"]))\n",
    "            out[\"asset_ema\"]  = (None if pd.isna(row[\"asset_ema\"]) else float(row[\"asset_ema\"]))\n",
    "            out[\"asset_last3\"]= (None if pd.isna(row[\"asset_last3\"]) else float(row[\"asset_last3\"]))\n",
    "\n",
    "        # Cohort fallback needs asset attributes\n",
    "        if df_context is not None and out[\"cohort_mean\"] is None:\n",
    "            ctx = df_context.copy()\n",
    "            ctx[ASSET_COL] = ctx[ASSET_COL].astype(str)\n",
    "            rows = ctx.loc[ctx[ASSET_COL] == sid]\n",
    "            if len(rows):\n",
    "                latest = rows.sort_values(\"_event_time\").iloc[-1]\n",
    "                cmean = self._lookup_cohort_mean(latest)\n",
    "                if cmean is not None:\n",
    "                    out[\"cohort_mean\"] = cmean\n",
    "\n",
    "        # Prediction policy:\n",
    "        # Prefer per-asset MEAN if any history exists (ensures per-ID differences)\n",
    "        if out[\"n_samples\"] >= 1 and out[\"asset_mean\"] is not None:\n",
    "            out[\"prediction_days\"] = out[\"asset_mean\"]\n",
    "            out[\"strategy\"] = \"asset_mean(n>=1)\"\n",
    "        elif out[\"cohort_mean\"] is not None:\n",
    "            out[\"prediction_days\"] = float(out[\"cohort_mean\"])\n",
    "            out[\"strategy\"] = \"cohort_mean\"\n",
    "        else:\n",
    "            out[\"prediction_days\"] = float(out[\"global_mean\"])\n",
    "            out[\"strategy\"] = \"global_mean\"\n",
    "\n",
    "        return out\n",
    "\n",
    "    def predict(self, system_ids: List[Any], df_context: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        rows = [self.predict_single(sid, df_context=df_context) for sid in system_ids]\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "# -----------------------\n",
    "# Train the model and demo predictions\n",
    "# -----------------------\n",
    "def main():\n",
    "    assert EXCEL_PATH.exists(), f\"File not found: {EXCEL_PATH}\"\n",
    "    df = pd.read_excel(EXCEL_PATH)\n",
    "\n",
    "    # Prepare timeline & normalize IDs (adds _event_time)\n",
    "    df = ensure_event_time(df)\n",
    "\n",
    "    # Fit\n",
    "    model = MTBFModel(cohort_cols=COHORT_COLS).fit(df)\n",
    "\n",
    "    print(\"\\n[MODEL] Learned statistics\")\n",
    "    print(f\"  Global MTBF mean (days): {model.global_mean:.2f}\" if not np.isnan(model.global_mean) else \"  Global MTBF mean: NaN\")\n",
    "    print(f\"  Assets with history (n_samples>=1): {int((model.asset_stats['n_samples'] >= 1).sum())}\")\n",
    "\n",
    "    for c in COHORT_COLS:\n",
    "        if c in model.cohort_stats:\n",
    "            print(f\"  Cohort table: {c} (groups={len(model.cohort_stats[c])})\")\n",
    "\n",
    "    # Demo predictions for first 5 IDs in the data\n",
    "    demo_ids = list(df[ASSET_COL].astype(str).dropna().unique()[:10])\n",
    "    preds = model.predict(demo_ids, df_context=df)\n",
    "\n",
    "    print(\"\\n[PREDICTIONS] MTBF days by System ID (per-asset if available):\")\n",
    "    print(preds.to_string(index=False))\n",
    "\n",
    "    #Example callable:\n",
    "    def predict_mtbf(system_id: str) -> float:\n",
    "        return float(model.predict([system_id], df_context=df).iloc[0][\"prediction_days\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a4835f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T18:26:00.549232Z",
     "iopub.status.busy": "2025-10-22T18:26:00.548850Z",
     "iopub.status.idle": "2025-10-22T18:26:00.554435Z",
     "shell.execute_reply": "2025-10-22T18:26:00.553139Z"
    },
    "papermill": {
     "duration": 0.01334,
     "end_time": "2025-10-22T18:26:00.556125",
     "exception": false,
     "start_time": "2025-10-22T18:26:00.542785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_mtbf(system_id: str) -> float:\n",
    "        return float(model.predict([system_id], df_context=df).iloc[0][\"prediction_days\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1175033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T18:26:00.567772Z",
     "iopub.status.busy": "2025-10-22T18:26:00.567381Z",
     "iopub.status.idle": "2025-10-22T18:26:00.573979Z",
     "shell.execute_reply": "2025-10-22T18:26:00.572845Z"
    },
    "papermill": {
     "duration": 0.01457,
     "end_time": "2025-10-22T18:26:00.575709",
     "exception": false,
     "start_time": "2025-10-22T18:26:00.561139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ASSET_COL = \"Asset_System_ID\"\n",
    "\n",
    "# Must exist from training:\n",
    "# - model       (your sklearn Pipeline)\n",
    "# - train_df    (engineered features table)\n",
    "# - X_cols      (the exact list of feature columns used to train `model`)\n",
    "\n",
    "def predict_mtbf_sklearn(system_id):\n",
    "    sid = str(system_id).strip()\n",
    "\n",
    "    # Pick the most recent feature snapshot for this asset\n",
    "    rows = (train_df[train_df[ASSET_COL].astype(str) == sid]\n",
    "            .sort_values(\"_event_time\"))\n",
    "    if rows.empty:\n",
    "        raise ValueError(f\"No engineered feature row found for System ID {sid}. \"\n",
    "                         f\"Use the lookup model (`mtbf_model`) or compute features for this asset.\")\n",
    "    \n",
    "    X = rows.iloc[[-1]][X_cols]   # keep as DataFrame with expected columns\n",
    "    yhat = model.predict(X)[0]    # sklearn pipeline\n",
    "    print(\"The MTBF is =\", yhat, \"days\")\n",
    "    pmint = 0.8*yhat\n",
    "    print(\"Suggested PM Interval =\", pmint, \"days\")\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b2a76f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T18:26:00.587709Z",
     "iopub.status.busy": "2025-10-22T18:26:00.587101Z",
     "iopub.status.idle": "2025-10-22T18:26:00.706576Z",
     "shell.execute_reply": "2025-10-22T18:26:00.705411Z"
    },
    "papermill": {
     "duration": 0.127773,
     "end_time": "2025-10-22T18:26:00.708419",
     "exception": false,
     "start_time": "2025-10-22T18:26:00.580646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MTBF is = 102.16887256547624 days\n",
      "Suggested PM Interval = 81.73509805238099 days\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "predict_mtbf_sklearn(\"8302160822967\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74443757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T18:26:00.720146Z",
     "iopub.status.busy": "2025-10-22T18:26:00.719817Z",
     "iopub.status.idle": "2025-10-22T18:26:17.357402Z",
     "shell.execute_reply": "2025-10-22T18:26:17.356305Z"
    },
    "papermill": {
     "duration": 16.645314,
     "end_time": "2025-10-22T18:26:17.359054",
     "exception": false,
     "start_time": "2025-10-22T18:26:00.713740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating report for 158 system IDs...\n",
      "Processing 0/158...\n",
      "Processing 100/158...\n",
      "\n",
      "[SUCCESS] Report generated: MTBF_Report_All_Systems.xlsx\n",
      "Total records: 158\n",
      "\n",
      "First 10 records:\n",
      "     System_ID  MTBF_days  Suggested_PM_Interval_days\n",
      " 8302160822967     102.17                       81.74\n",
      "83021600005521      60.35                       48.28\n",
      "83021600015312     146.23                      116.99\n",
      "83021600098222      39.37                       31.50\n",
      "83021600100022      24.81                       19.84\n",
      "83021600103323      29.09                       23.27\n",
      "83021600108822      60.83                       48.66\n",
      "83021600108923      13.02                       10.42\n",
      "83021600184824      72.76                       58.21\n",
      "83021600420416      99.81                       79.84\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Generate Excel Report for All System IDs\n",
    "# -----------------------\n",
    "\n",
    "def generate_mtbf_report():\n",
    "    # Get all unique system IDs from the training data\n",
    "    all_system_ids = train_df[ASSET_COL].astype(str).unique()\n",
    "    print(f\"[INFO] Generating report for {len(all_system_ids)} system IDs...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, system_id in enumerate(all_system_ids):\n",
    "        if i % 100 == 0:  # Progress indicator\n",
    "            print(f\"Processing {i}/{len(all_system_ids)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Get the most recent feature snapshot for this asset\n",
    "            rows = (train_df[train_df[ASSET_COL].astype(str) == system_id]\n",
    "                    .sort_values(\"_event_time\"))\n",
    "            \n",
    "            if not rows.empty:\n",
    "                X = rows.iloc[[-1]][X_cols]   # Most recent record\n",
    "                mtbf = model.predict(X)[0]    # sklearn pipeline prediction\n",
    "                pm_interval = 0.8 * mtbf      # 80% of MTBF\n",
    "                \n",
    "                # Round to 2 decimal places\n",
    "                mtbf_rounded = round(mtbf, 2)\n",
    "                pm_interval_rounded = round(pm_interval, 2)\n",
    "                \n",
    "                results.append({\n",
    "                    'System_ID': system_id,\n",
    "                    'MTBF_days': mtbf_rounded,\n",
    "                    'Suggested_PM_Interval_days': pm_interval_rounded\n",
    "                })\n",
    "            else:\n",
    "                print(f\"[WARN] No data found for System ID: {system_id}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process System ID {system_id}: {e}\")\n",
    "            # You can choose to skip or add placeholder values\n",
    "            results.append({\n",
    "                'System_ID': system_id,\n",
    "                'MTBF_days': 'Error',\n",
    "                'Suggested_PM_Interval_days': 'Error'\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to Excel\n",
    "    output_file = \"MTBF_Report_All_Systems.xlsx\"\n",
    "    results_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\n[SUCCESS] Report generated: {output_file}\")\n",
    "    print(f\"Total records: {len(results_df)}\")\n",
    "    print(\"\\nFirst 10 records:\")\n",
    "    print(results_df.head(10).to_string(index=False))\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Generate the report\n",
    "mtbf_report = generate_mtbf_report()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8549861,
     "sourceId": 13468599,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.34248,
   "end_time": "2025-10-22T18:26:19.985495",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-22T18:25:16.643015",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
